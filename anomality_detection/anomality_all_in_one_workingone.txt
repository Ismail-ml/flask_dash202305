Formula = {'2G': {
    'Call Setup SR': "100*(1-df['cssr_num1']/df['cssr_den1'])*df['cssr_num2']/df['cssr_den2']*df['cssr_num3']/df['cssr_den3']",
    'Call Drop Rate': "df['drop_rate_num']/df['drop_rate_den']*100",
    'Call Block Rate': "df['call_block_rate_num']/df['call_block_rate_den']*100",
    'SDCCH Drop Rate': "df['sdcch_drop_rate_num']/df['sdcch_drop_rate_den']*100",
    'SDCCH Block Rate': "df['sdcch_block_rate_num']/df['sdcch_block_rate_den']*100",
    'Handover SR': "df['hosr_num']/df['hosr_den']*100",
    'Cell Availability excl blck': "(df['cell_avail_num']+df['cell_avail_blck_num'])/(df['cell_avail_den']-df['cell_avail_blck_den'])*100",
    'TCH Availability': "df['tch_avail_num']/df['tch_avail_den']*100",
    'SDCCH Availability': "df['sdcch_avail_num']/df['sdcch_avail_den']*100",
    'TBF Est SR': "df['tbf_est_sr_num']/df['tbf_est_sr_den']*100",
    'TBF Drop Rate': "df['tbf_drop_rate_num']/df['tbf_drop_rate_den']*100",
    'CS Traffic, Erl': "df['cs_traffic_erl']",
    'PS Traffic, MB': "df['ps_traffic_mb']",
    'Combined Thrp, Kbps': "df['comb_thrp_num']/df['comb_thrp_den']",
    'Call Setup Fails': "df['cssr_num1']+df['cssr_den2']+df['cssr_den3']-df['cssr_num2']-df['cssr_num3']",
    'Call Setup Den': "df['cssr_den1']+df['cssr_den2']+df['cssr_den3']",
    'Call Drops': "df['drop_rate_num']",
    'Call Blocks': "df['call_block_rate_num']",
    'SDCCH Drops': "df['sdcch_drop_rate_num']",
    'SDCCH Blocks': "df['sdcch_block_rate_num']",
    'TBF Drops': "df['tbf_drop_rate_num']",
    'TBF Est fails': "df['tbf_est_sr_den']-df['tbf_est_sr_num']",
    'Handover fails': "df['hosr_den']-df['hosr_num']", 'Handover attempts': "df['hosr_den']"
},
    '3G': {
        'Voice Call Setup SR': "100*df['voice_sr_num1']/df['voice_sr_den1']*df['voice_sr_num2']/df['voice_sr_den2']",
        'Voice Call DR': "100*df['voice_dr_num']/df['voice_dr_den']",
        'HSDPA RAB SR': "100*df['hsdpa_sr_num']/df['hsdpa_sr_den']",
        'HSUPA RAB SR': "100*df['hsupa_sr_num']/df['hsupa_sr_den']",
        'HSDPA Drop Rate': "100*df['hsdpa_dr_num']/df['hsdpa_dr_den']",
        'HSUPA Drop Rate': "100*df['hsupa_dr_num']/df['hsupa_dr_den']",
        'CS Soft HOSR': "100*df['cs_sho_ho_num']/df['cs_sho_ho_den']",
        'PS Soft HOSR': "100*df['ps_sho_ho_num']/df['ps_sho_ho_den']",
        'CS IRAT HOSR': "100*df['cs_inter_rat_ho_num']/df['cs_inter_rat_ho_den']",
        'CS InterFreq HOSR': "100*df['cs_inter_freq_ho_num']/df['cs_inter_freq_ho_den']",
        'PS InterFreq HOSR': "100*df['ps_inter_freq_ho_num']/df['ps_inter_freq_ho_den']",
        'Cell Availability excl blck': "100*(df['cell_avail_num']+df['cell_avail_blck_num'])/(df['cell_avail_den']-df['cell_avail_blck_den'])",
        'Average HSDPA user thrp, Kbps': "df['hsdpa_thrp_num']/df['hsdpa_thrp_den']",
        'CS Traffic, Erl': "df['cs_traf']",
        'PS Traffic, MB': "df['ps_traf']",
        'Call Setup fails': "df['voice_sr_den1']+df['voice_sr_den2']-df['voice_sr_num1']-df['voice_sr_num2']",
        'Call drops': "df['voice_dr_num']",
        'Call Setup attempts': "df['voice_sr_den1']+df['voice_sr_den2']",
        'HSDPA Setup fails': "df['hsdpa_sr_den']-df['hsdpa_sr_num']",
        'HSDPA Setup attempts': "df['hsdpa_sr_den']",
        'HSDPA drops': "df['hsdpa_dr_num']",
        'HSUPA Setup fails': "df['hsupa_sr_den']-df['hsupa_sr_num']",
        'HSUPA Setup attempts': "df['hsupa_sr_den']",
        'HSUPA drops': "df['hsupa_dr_num']",
        'CS Soft HO fails': "df['cs_sho_ho_den']-df['cs_sho_ho_num']",
        'CS Soft HO attempts': "df['cs_sho_ho_den']",
        'PS Soft HO fails': "df['ps_sho_ho_den']-df['ps_sho_ho_num']",
        'PS Soft HO attempts': "df['ps_sho_ho_den']",
        'CS IRAT HO attempts': "df['cs_inter_rat_ho_den']",
        'CS IRAT HO fails': "df['cs_inter_rat_ho_den']-df['cs_inter_rat_ho_num']",
        'CS InterFreq HO attempts': "df['cs_inter_freq_ho_den']",
        'CS InterFreq HO fails': "df['cs_inter_freq_ho_den']-df['cs_inter_freq_ho_num']",
        'PS InterFreq HO attempts': "df['ps_inter_freq_ho_den']",
        'PS InterFreq HO fails': "df['ps_inter_freq_ho_den']-df['ps_inter_freq_ho_num']"},
    '4G': {'RRC Setup SR': "100*df['rrc_sr_num']/df['rrc_sr_den']",
           'RAB Setup SR': "100*df['rab_sr_num']/df['rab_sr_den']",
           'Session Setup SR': "100*df['rrc_sr_num']/df['rrc_sr_den']*df['rab_sr_num']/df['rab_sr_den']",
           'CSFB SR': "100*df['csfb_sr_num']/df['csfb_sr_den']",
           'Session Drop Rate': "100*df['dcr_num']/df['dcr_den']",
           'Cell Availability excl blck': "100*(df['cell_avail_num']+df['cell_avail_blck_num'])/(df['cell_avail_den']-df['cell_avail_blck_den'])",
           'IntraFreq HOSR': "100*df['intra_freq_ho_num']/df['intra_freq_ho_den']",
           '4G-3G IRAT HOSR': "100*df['irat_ho_num']/df['irat_ho_den']",
           'DL Traffic, MB': "df['dl_ps_traf']",
           'UL Traffic, MB': "df['ul_ps_traf']",
           'Total Traffic, MB': "df['dl_ps_traf']+df['ul_ps_traf']",
           'DL Throughput, Kbps': "df['dl_thrp_num']/df['dl_thrp_den']",
           'UL Throughput, Kbps': "df['ul_thrp_num']/df['ul_thrp_den']",
           'RRC Setup fails': "df['rrc_sr_den']-df['rrc_sr_num']",
           'RRC Setup attempts': "df['rrc_sr_den']",
           'RAB Setup fails': "df['rab_sr_den']-df['rab_sr_num']",
           'RAB Setup attempts': "df['rab_sr_den']",
           'Session drops': "df['dcr_num']",
           'Session Setup fails': "df['rrc_sr_den']-df['rrc_sr_num']+df['rab_sr_den']-df['rab_sr_num']",
           'CSFB fails': "df['csfb_sr_den']-df['csfb_sr_num']",
           'CSFB attempts': "df['csfb_sr_den']",
           'IntraFreq HO fails': "df['intra_freq_ho_den']-df['intra_freq_ho_num']",
           'IntraFreq HO attempts': "df['intra_freq_ho_den']",
           '4G-3G IRAT HO fails': "df['irat_ho_den']-df['irat_ho_num']",
           '4G-3G IRAT HO attempts': "df['irat_ho_den']"}}

Komekci_kpi = {'Call Setup SR': 'Call Setup Fails', 'Call Drop Rate': 'Call Drops', 'Call Block Rate': 'Call Blocks',
               'SDCCH Drop Rate': 'SDCCH Drops', 'SDCCH Block Rate': 'SDCCH Blocks', 'Handover SR': 'Handover fails',
               'Cell Availability excl blck': 'Cell Availability excl blck', 'TCH Availability': 'TCH Availability',
               'SDCCH Availability': 'SDCCH Availability', 'TBF Est SR': 'TBF Est fails', 'TBF Drop Rate': 'TBF Drops',
               'Combined Thrp, Kbps': 'PS Traffic, MB',
               'Voice Call Setup SR': 'Call Setup fails', 'Voice Call DR': 'Call drops',
               'HSDPA RAB SR': 'HSDPA Setup fails',
               'HSUPA RAB SR': 'HSUPA Setup fails', 'HSDPA Drop Rate': 'HSDPA drops', 'HSUPA Drop Rate': 'HSUPA drops',
               'CS Soft HOSR': 'CS Soft HO fails', 'PS Soft HOSR': 'PS Soft HO fails',
               'CS IRAT HOSR': 'CS IRAT HO fails',
               'CS InterFreq HOSR': 'CS InterFreq HO fails', 'PS InterFreq HOSR': 'PS InterFreq HO fails',
               'Cell Availability excl blck': 'Cell Availability excl blck',
               'Average HSDPA user thrp, Kbps': 'PS Traffic, MB',
               'CSFB SR': 'CSFB fails', 'Session Drop Rate': 'Session drops', 'Session Setup SR': 'Session Setup fails',
               'Cell Availability excl blck': 'Cell Availability excl blck', 'IntraFreq HOSR': 'IntraFreq HO fails',
               '4G-3G IRAT HOSR': '4G-3G IRAT HO fails', 'DL Throughput, Kbps': 'DL Traffic, MB',
               'UL Throughput, Kbps': 'UL Traffic, MB',
               }
import pandas as pd
import os

os.chdir(r'/home/ismayil/flask_dash/support_files')
import datetime
from datetime import datetime as dt

needed = dt.strptime(dt.strftime(dt.now() - datetime.timedelta(hours=1), '%d.%m.%y %H:00'), '%d.%m.%y %H:00')
hh = pd.date_range(end=needed, periods=15, freq='24H')

# No need anymore
# os.chdir(r'/home/ismayil/flask_dash/support_files/anomality_detection')
# for file in os.listdir():
#    if dt.strftime(needed, '%H:00') == '00:00':
#        if 'for' in file:
#            os.remove(file)
#    elif 'trend' in file:
#        if file not in ['anomalies_trend_' + dt.strftime(pd.date_range(end=needed, periods=24, freq='H')[i], '%Y-%m-%d %H') + '.csv' for i in range(24)]:
#            os.remove(file)
# os.chdir(r'/home/ismayil/flask_dash/support_files')
################

files = pd.date_range(end=needed, periods=15, freq='24H').strftime("%B_%Y").tolist()
print(hh)
print(set(files))
techs = {'2G': [['Date', 'Site_name', 'Vendor', 'BSC_name', 'Region'], '/twoG'],
         '3G': [['Date', 'Site_name', 'Vendor', 'RNC_name', 'Region'], '/threeG'],
         '4G': [['Date', 'Site_name', 'Vendor', 'Region'], '/fourG']}
final = []
trends = []
for tech in techs.keys():
    to_concat = []
    for i in set(files):
        if os.path.isfile(os.path.join(r'/disk2/support_files/archive', i + '.h5')):
            to_concat.append(pd.read_hdf(os.path.join(r'/disk2/support_files/archive', i + '.h5'), techs[tech][1], where='Date in hh'))
    df = pd.concat(to_concat).groupby(techs[tech][0]).sum().reset_index()

    for i in Formula[tech].keys():
        df[i] = eval(Formula[tech][i])
    df2 = df[techs[tech][0] + list(Formula[tech].keys())]
    l = df2.columns.get_loc('Region') + 1
    # most efficient way of finding threshold
    filt = [i for i in df2.columns if
            (('SR' in i) | ('thrp' in i) | ('Traffic' in i) | ('Thr' in i) | ('Availability' in i))]
    Q1 = df2.groupby('Site_name').quantile(0.25)
    Q3 = df2.groupby('Site_name').quantile(0.75)
    thr = (Q1[filt] - 1.5 * (Q3[filt] - Q1[filt])).merge(Q3[df2.columns[l:][~df2.columns[l:].isin(filt)]] + 1.5 * (
            Q3[df2.columns[l:][~df2.columns[l:].isin(filt)]] - Q1[df2.columns[l:][~df2.columns[l:].isin(filt)]]),
                                                         left_index=True, right_index=True).iloc[:, 1:]
    df4 = df2.melt(id_vars=df2.columns[:l], value_vars=df2.columns[l:]).merge(
        thr.reset_index().melt(id_vars='Site_name', value_name='threshold'), on=['Site_name', 'variable'])
    trend = df2.melt(id_vars=df2.columns[:l], value_vars=df2.columns[l:])
    trend.insert(1, 'tech', tech)
    if tech == '4G': trend.insert(4, 'NE', 'eNodeB')
    trend.rename(columns={trend.columns[4]: 'NE'}, inplace=True)
    trends.append(trend)
    # most efficient way
    # df4.loc[df4['variable'].isin(['Cell Availability excl blck','TCH Availability','SDCCH Availability']),'threshold']=100
    df4.loc[(df4['value'] < df4['threshold']) & (df4['variable'].isin(filt)), 'status'] = -1
    df4.loc[(df4['value'] > df4['threshold']) & (df4['variable'].isin(filt)), 'status'] = 1
    df4.loc[(df4['value'] < df4['threshold']) & (~df4['variable'].isin(filt)), 'status'] = 1
    df4.loc[(df4['value'] > df4['threshold']) & (~df4['variable'].isin(filt)), 'status'] = -1
    df4.loc[df4['value'] == df4['threshold'], 'status'] = 0
    # df4.insert(1,'Tech',tech)
    df4['delta'] = df4['threshold'] - df4['value']
    df4.insert(1, 'tech', tech)
    if tech == '4G': df4.insert(4, 'NE', 'eNodeB')
    df4.rename(columns={df4.columns[4]: 'NE'}, inplace=True)
    final.append(df4)
df4 = pd.concat(final)
# pd.concat(trends).to_csv('anomality_detection/anomalies_trend_'+str(needed).split(':')[0]+'.csv',index=False)
pd.concat(trends).to_hdf('/disk2/support_files/archive/anomalies.h5', '/trend',
                         format='table', complevel=5, data_columns=trend.columns)
# df4[df4['Date']==df4['Date'].unique()[-1]].to_csv('anomality_detection/anomalies_for_'+str(needed).split(':')[0]+'.csv',index=False)
df4[df4['Date'] == df4['Date'].unique()[-1]].to_hdf(
    '/disk2/support_files/archive/anomalies.h5', '/for', format='table', complevel=5)
########################

needed = dt.strptime(dt.strftime(dt.now() - datetime.timedelta(hours=1), '%d.%m.%y %H:00'), '%d.%m.%y %H:00')
hh = pd.date_range(end=needed, periods=30, freq='24H')

files = pd.date_range(end=needed, periods=30, freq='24H').strftime("%B_%Y").tolist()
print(hh)
print(set(files))
techs = {'2G': [['Date', 'Vendor', 'BSC_name'], '/twoG/bsc'],
         '3G': [['Date', 'Vendor', 'RNC_name'], '/threeG/bsc'],
         '4G': [['Date', 'Vendor', 'Region'], '/fourG/bsc']}
final = []
trends2 = []
for tech in techs.keys():
    to_concat = []
    for i in set(files):
        if os.path.isfile(os.path.join(r'/disk2/support_files/archive', i + '.h5')):
            to_concat.append(pd.read_hdf(os.path.join(r'/disk2/support_files/archive', i + '.h5'), techs[tech][1], where='Date in hh'))
    df = pd.concat(to_concat).groupby(techs[tech][0]).sum().reset_index()

    for i in Formula[tech].keys():
        df[i] = eval(Formula[tech][i])
    df2 = df[techs[tech][0] + list(Formula[tech].keys())]
    l = 3
    # most efficient way of finding threshold
    filt = [i for i in df2.columns if
            (('SR' in i) | ('thrp' in i) | ('Traffic' in i) | ('Thr' in i) | ('Availability' in i))]
    Q1 = df2.groupby(techs[tech][0][2]).quantile(0.25)
    Q3 = df2.groupby(techs[tech][0][2]).quantile(0.75)
    thr = (Q1[filt] - 1.5 * (Q3[filt] - Q1[filt])).merge(Q3[df2.columns[l:][~df2.columns[l:].isin(filt)]] + 1.5 * (
            Q3[df2.columns[l:][~df2.columns[l:].isin(filt)]] - Q1[df2.columns[l:][~df2.columns[l:].isin(filt)]]),
                                                         left_index=True, right_index=True).iloc[:, 1:]
    df4 = df2.melt(id_vars=df2.columns[:l], value_vars=df2.columns[l:]).merge(
        thr.reset_index().melt(id_vars=techs[tech][0][2], value_name='threshold'), on=[techs[tech][0][2], 'variable'])
    trend = df2.melt(id_vars=df2.columns[:l], value_vars=df2.columns[l:])
    trend.insert(1, 'tech', tech)
    trend.rename(columns={trend.columns[3]: 'NE'}, inplace=True)
    trends2.append(trend)

    # most efficient way
    df4.loc[(df4['value'] < df4['threshold']) & (df4['variable'].isin(filt)), 'status'] = -1
    df4.loc[(df4['value'] > df4['threshold']) & (df4['variable'].isin(filt)), 'status'] = 1
    df4.loc[(df4['value'] < df4['threshold']) & (~df4['variable'].isin(filt)), 'status'] = 1
    df4.loc[(df4['value'] > df4['threshold']) & (~df4['variable'].isin(filt)), 'status'] = -1
    df4.loc[df4['value'] == df4['threshold'], 'status'] = 0
    # df4.insert(1,'Tech',tech)
    df4['delta'] = df4['threshold'] - df4['value']
    df4.insert(1, 'tech', tech)
    df4.rename(columns={df4.columns[3]: 'NE'}, inplace=True)
    final.append(df4)
df4 = pd.concat(final)
# df4[df4['Date']==df4['Date'].unique()[-1]].to_csv('anomality_detection/anomalies(higher)_for_'+str(needed).split(':')[0]+'.csv',index=False)
df4[df4['Date'] == df4['Date'].unique()[-1]].to_hdf(
    '/disk2/support_files/archive/anomalies.h5', '/for_higher', format='table',
    complevel=5)
pd.concat(trends2).to_hdf('/disk2/support_files/archive/anomalies.h5', '/trend_higher',
                          format='table', complevel=5, data_columns=trend.columns)

os.chdir(r'/home/ismayil/flask_dash/support_files/anomality_detection')
files = sorted(os.listdir(), key=os.path.getctime)[-3:-1]
print(sorted(os.listdir(), key=os.path.getctime)[-3:-1])
for i in ['/for', '/for_higher']:
    if 'higher' in i:
        level = 'higher'
        lookup = 'NE'
    else:
        level = 'site'
        lookup = 'Site_name'
    anomalies = pd.read_hdf('/disk2/support_files/archive/anomalies.h5',
                            '/' + level + '_level')
    df = pd.read_hdf('/disk2/support_files/archive/anomalies.h5', i)
    anomalies = df[df['status'] == -1].merge(anomalies[[lookup, 'variable', 'status', 'c_days']],
                                             on=[lookup, 'variable'], how='left')
    anomalies.loc[anomalies['c_days'].notnull(), 'c_days'] += 1
    anomalies.loc[anomalies['c_days'].isnull(), 'c_days'] = 1
    anomalies.rename(columns={'status_x': 'status'}, inplace=True)
    anomalies.drop(columns='status_y', inplace=True)
    anomalies = anomalies[anomalies['variable'].isin(
        ['Call Setup SR', 'Call Drop Rate', 'Call Block Rate', 'SDCCH Drop Rate', 'SDCCH Block Rate',
         'Handover SR', 'Cell Availability excl blck', 'TCH Availability', 'SDCCH Availability', 'TBF Est SR',
         'TBF Drop Rate',
         'Voice Call Setup SR', 'Voice Call DR', 'HSDPA RAB SR', 'HSUPA RAB SR', 'HSDPA Drop Rate', 'HSUPA Drop Rate',
         'CS Soft HOSR',
         'PS Soft HOSR', 'CS IRAT HOSR', 'CS InterFreq HOSR', 'PS InterFreq HOSR', 'Average HSDPA user thrp, Kbps',
         'Session Setup SR',
         'CSFB SR', 'Session Drop Rate', 'IntraFreq HOSR', '4G-3G IRAT HOSR', 'DL Throughput, Kbps',
         'UL Throughput, Kbps'])]
    ####################
    if level == 'site':
        df2 = pd.concat(trends)
        nese = df2[df2['variable'].isin(Komekci_kpi.values())].pivot_table(index='Site_name', columns='variable',
                                                                           values='value', aggfunc='mean')
        merged = anomalies.merge(nese, left_on='Site_name', right_index=True, how='left')
        for i in Komekci_kpi.keys():
            merged.loc[merged['variable'] == i, 'Fails_and_drops_average_15_day'] = merged.loc[
                merged['variable'] == i, Komekci_kpi[i]]
        merged.drop(columns=Komekci_kpi.values(), inplace=True)
        merged = merged.merge(
            df2[df2['Date'] == df2['Date'].unique()[-1]].pivot_table(index='Site_name', columns='variable',
                                                                     values='value',
                                                                     aggfunc='mean'), left_on='Site_name',
            right_index=True,
            how='left')
        for i in Komekci_kpi.keys():
            merged.loc[merged['variable'] == i, 'Fails_and_drops_last_hour'] = merged.loc[
                merged['variable'] == i, Komekci_kpi[i]]
        merged = merged[['Date', 'tech', 'Site_name', 'Vendor', 'NE', 'Region', 'variable',
                         'value', 'threshold', 'Fails_and_drops_average_15_day', 'Fails_and_drops_last_hour', 'status',
                         'delta', 'c_days',
                         ]]
        anomalies = merged.merge(df[df['variable'] == 'Cell Availability excl blck'][['Site_name', 'value']].rename(
            columns={'value': 'Cell Availability'}), on='Site_name', how='left')[
            ['Date', 'tech', 'Vendor', 'Site_name', 'NE', 'Region', 'variable',
             'value', 'threshold', 'Fails_and_drops_average_15_day', 'Fails_and_drops_last_hour', 'Cell Availability',
             'c_days', 'status']]

    else:
        df2 = pd.concat(trends2)
        nese = df2[df2['variable'].isin(Komekci_kpi.values())].pivot_table(index='NE', columns='variable',
                                                                           values='value', aggfunc='mean')
        merged = anomalies.merge(nese, left_on='NE', right_index=True, how='left')
        for i in Komekci_kpi.keys():
            merged.loc[merged['variable'] == i, 'Fails_and_drops_average_15_day'] = merged.loc[
                merged['variable'] == i, Komekci_kpi[i]]
        merged.drop(columns=Komekci_kpi.values(), inplace=True)
        merged = merged.merge(
            df2[df2['Date'] == df2['Date'].unique()[-1]].pivot_table(index='NE', columns='variable',
                                                                     values='value',
                                                                     aggfunc='mean'), left_on='NE',
            right_index=True,
            how='left')
        for i in Komekci_kpi.keys():
            merged.loc[merged['variable'] == i, 'Fails_and_drops_last_hour'] = merged.loc[
                merged['variable'] == i, Komekci_kpi[i]]
        merged = merged[['Date', 'tech', 'Vendor', 'NE', 'variable',
                         'value', 'threshold', 'Fails_and_drops_average_15_day', 'Fails_and_drops_last_hour', 'status',
                         'delta', 'c_days',
                         ]]
        anomalies = merged.merge(df[df['variable'] == 'Cell Availability excl blck'][['NE', 'value']].rename(
            columns={'value': 'Cell Availability'}), on='NE', how='left')[
            ['Date', 'tech', 'Vendor', 'NE', 'variable',
             'value', 'threshold', 'Fails_and_drops_average_15_day', 'Fails_and_drops_last_hour', 'Cell Availability',
             'c_days', 'status']]

    # .strftime("%B_%Y")
    ###################
    anomalies.to_hdf('/disk2/support_files/archive/anomalies.h5', '/' + level + '_level',
                     format='table', complevel=5)
    # anomalies.to_csv('anomalies_' + level + '_level.csv', index=False)
    # anomalies.to_csv('archive/anomalies_' + level + '_level_' + str(needed).split(':')[0] + '.csv', index=False)
    anomalies.to_hdf(
        '/disk2/support_files/archive/' + anomalies['Date'][0].strftime(
            "%B_%Y") + '_anomalies.h5', '/' + level,
        append=True, format='table', data_columns=anomalies.columns, complevel=5)
